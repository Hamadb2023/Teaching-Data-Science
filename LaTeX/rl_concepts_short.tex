%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Concepts}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Questions}

\begin{itemize}
\item What is the probability that an Agent will select as specific Action while being in some State?
\item How good the proposed Action is at the given State? as this 'goodness' decides the Reward.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Agent}

A system that is:

\begin{itemize}
\item situated in an environment
\item is capable of perceiving its environment,
\item is capable of acting in its environment
\item with the goal of satisfying its design objectives
\item e.g. Vacuum cleaning robot is agent, in house Environment, with goal to clan the house.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Agent}

\begin{itemize}
\item All the rest of the world in which the Agent operates
\item Fully observable: Agent can access to states of the Environment
\item Partially observable: Agent can access only some/currently-see-able states of the Environment
\item Deterministic vs Non-Deterministic
\item Episodic vs Sequential
\item Discrete vs Continuous
\item Static vs Dynamic
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Agent}

At each step, the agent:
\begin{itemize}
\item Executes action
\item Observe new state
\item Receive reward
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Environment and Actions}

\begin{itemize}
\item Fully Observable (Chess) vs Partially Observable (Poker)
\item Single Agent (Atari) vs Multi Agent (DeepTraffic)
\item Deterministic (Cart Pole) vs Stochastic (DeepTraffic)
\item Static (Chess) vs Dynamic (DeepTraffic)
\item Discrete (Chess) vs Continuous (Cart Pole)

\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Policy}

\begin{itemize}
\item Policy is a function that maps State to probability of choosing a particular Action at that State.
\item Policy can be a lookup tables, a simple function or complex functions computations.
\item Policy is what the Agent learns.
\item Generally, the Policy may be Stochastic, specifying probabilities for each action.
\item Denoted as $\pi (a|s)$ where,
	\begin{itemize}
	\item $\pi$ : Policy, a probability distribution of Action $a$ over each State $s$
	\item $a = A_t, a \in A$ for $s = S_t, s \in S$
	\end{itemize}

\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Reward}

\begin{itemize}
\item Reward is the goal.
\item At each time-step, Action of the Agent results in reward.
\item Reward is the primary basis for altering the Policy
\item Generally, the reward function may be stochastic, on State and Action.
\item Rewards are directly given by the Environment, but the values have been constantly estimated from the sequences of observations the Agent makes at each interaction. This will make the method(s) of estimating values efficiently the most important component of Reinforcement Learning algorithm.
\end{itemize}

\end{frame}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Formally}
% \begin{center}
% \includegraphics[width=0.6\linewidth,keepaspectratio]{rl1}
% \end{center}

% \begin{itemize}
% \item Environment has States
% \item Agent has a Policy with which it decides the Action.
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Whole Process}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl1}
\end{center}

\begin{itemize}
\item Environment is like a black box, it is poked with Action
\item Once an Action comes to the Environment, it changes its State and that changed State is communicated back to the Agent.
\item Along with that, the Environment also communicates if any Reward/Penalty was acquired or not.
\item Just by poking multiple times, the Agent learns (thats Machine Learning) and builds such a Policy-Strategy-recipes, that the next Action proposed will get a Reward rather than Penalty.
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{MDP}

\begin{itemize}
\item MDP defines how an Agent takes sequential Actions from a State to State.
\item MDP formulation: $(S,A,R,\lambda)$, where,
	\begin{itemize}
	\item $S$ : State space
	\item $A$ : Action space
	\item $R$ : Reward function
	\item $\lambda$ : Discount factor
	\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl2}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{MDP}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl2}
\end{center}

\begin{itemize}
\item Time steps $t = 0,1,2,3,\ldots$
\item At time step $t$, the Agent receives State, $S_t \in S$ and selects an Action $A_t \in A(S)$
\item One time step later $t+1$, the Agent receives Reward $R_{t+1} \in R$ and the Environment transitions to a new State $S_{t+1}$
\item Thus, MDP is a sequence : $S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3\ldots$
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Expected Returns}

\begin{itemize}
\item Agents goal is to maximize Expected Returns
\item Returns is the sum of future rewards.
\item Goal $G_t= R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_n = \sum_{t=1}^{n}R_t$
\item Agent cares more about immediate rewards than the future ones. The Goal is updated as \ldots
\item Goal $G_t= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^{n-1}R_n = \sum_{t=1}^{n}\gamma^{n-1}R_t$
\item At $\gamma = 0$ the Agent will be completely myopic. Looks at only immediate Reward.
\item At $\gamma = 1$ the Agent does not discount future Rewards but treats them equally.
\item $\gamma$ is typically between 0 and 1.

\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Value Function}

\begin{itemize}
\item Value Functions are functions of States or State-Action pairs
\item It decides whats good in the long run. Even if a State might yield a low immediate reward, it still can have a high value because it is regularly followed by other States that yield higher rewards.
\item They estimate how good it is for this Agent to be in this State
\item How good it is for this Agent to perform a given Action in this given State, considering the expected returns.
\item Reward depends on what Action the Agent takes at the given State.
\item Value functions are defined with respect to the Policy.
\item The Agent will seek Actions that bring States of highest Value, not highest reward. These States will lead to Actions that earn the greatest amount of reward over the long run.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Purpose}

An RL agent may be directly or indirectly trying to learn a:

\begin{itemize}
\item Policy: agent’s behavior function
\item Value function: how good is each state and/or action
\item Model: agent’s representation of the environment
\item Goal: Maximize Reward
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Path Generation}

For an Observation/State $O_t$, there are multiple possible Actions, say, $A_1, A_2, A_3, A_4$. They, once taken generate another set of Observations/State, respectively.

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl18}
\end{center}


{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Path Generation}

Policy function decides which of these Actions are most promising. Here $A_1, A_3$ are.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl19}
\end{center}


{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Path Generation}

Follow same procedure till end. 

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl20}
\end{center}


{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Path Generation}

Once it has all rewards, then it adjust the values (Q-Learning?).

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl21}
\end{center}


{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{State Value Function}

\begin{itemize}
\item State Value Functions $v_{\pi}$ gives the value of a State $s$ under given Policy $\pi$
\item How good is a State for the Agent following the Policy $\pi$
\end{itemize}

$v_{\pi}(s) = E_{\pi}[G_t | S_t = s] = E_{\pi}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s]$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Action Value Function, Q Function}

\begin{itemize}
\item Method to estimate value of an Action, and then selecting among pool of such available Actions.
\item True (actual) value of Action $a$ is $Q^*(a)$ and the estimated value after $t$ time-steps is $Q_t(a)$.
\item True value of an Action is the mean reward given that the Action is selected. Ratio of sum of rewards when $a$ taken prior to $t$, to number of tomes a taken prior to $t$. $Q_t(a) = = \frac{\sum_{i=1}^{t-1} R_i}{t-1}$
\item Action Value Functions $q_{\pi}$ gives the value of a State-Action pair $(s,a)$ 
\item Q - quality of taking a given Action in a given State to determine best Policy $\pi$ for the Agent.
\item For small problems, Q values are in form of a table, States as rows and Actions as columns.
\end{itemize}

\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Models}

% \begin{itemize}
% \item Markov Decision Process: MDP is a mathematical framework to describe an environment in RL and almost all RL problems can be formulated using MDPs. The outcome of deploying an action to a state doesn’t depend on previous actions or states but on current action and state.
% \item Q Learning – it’s a value-based model free approach for supplying information to intimate which action an agent should perform. It revolves around the notion of updating Q values which shows the value of doing action A in state S.
% \end{itemize}


% \end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Concepts}
% \begin{center}
% \includegraphics[width=0.6\linewidth,keepaspectratio]{rl31}

% \includegraphics[width=0.6\linewidth,keepaspectratio]{rl32}

% \end{center}

% {\tiny (Ref: Reinforcement Learning Example Using Python - Edureka)}

% \end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Concepts}
% \begin{center}
% \includegraphics[width=0.9\linewidth,keepaspectratio]{rl33}

% \end{center}

% {\tiny (Ref: Reinforcement Learning Example Using Python - Edureka)}

% \end{frame}




% \section[Opt]{Learning Optimum Policy}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{}
% \begin{center}
% {\Large Learning Optimum Policy}
% \end{center}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Optimal Policy}

$\pi \geq \pi' \iff v_{\pi}(s) \geq {v'}_{\pi}(s) \forall s \in S$

\begin{itemize}
\item A Policy is measured based on State Value function
\item So, whichever Policy has State Value function better at the given State, is considered better
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Optimal State Value Function}

$v_{*}(s) = max_{\pi} v_{\pi}(s) \forall s \in S$

$v_{*}$ gives the largest expected return achievable by any Policy $\pi$ for each State.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Optimal Action Value Function}

$q_{*}(s,a) = max_{\pi} q_{\pi}(s,a) \forall s \in S, a \in A(s)$

$q_{*}$ gives the largest expected return achievable by any Policy $\pi$ for each State-Action pair.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Bellman Optimality Equation for Action Value Function}

\begin{itemize}
\item According to Bellman Optimality Equation, $q_{*}$ must satisfy,

$q_{*}(s,a) = E[R{t+1} + \gamma max_{a'} q_{*}(s',a')]$

\item Optimal Policy is found by applying iterative RL algorithms to find the Action that maximizes $q_*$ for each State.

\item Calculating $q_*$ for each state is very cumbersome if the State space is very large. Some approximation is applied like Dynamic Programming, Monte Carlo Method and Temporal Difference.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{So, formally, RL}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl23}
\end{center}

{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}
\end{frame}

