%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Deep Q Learning}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Q Learning}

\begin{itemize}
\item Instead of Q table, if neural network is employed which takes a State as input and gives approximate Q values for different actions, then thats Deep Q learning
\item Useful especially if there are too many states and complexity.
\end{itemize}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl10}
\end{center}

{\tiny (Ref: Deep Q Network, a deep reinforcement learning approach - Nitin Mukesh)}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Q Learning}

Some times Experience Replay or Action Replay memory is used, where one calculation ie $(s_t,a_t,r_t,s_{t+1})$ tuple is stored in the memory. All such entries forms a training data, which can train the neural network.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl11}
\end{center}

{\tiny (Ref: Deep Q Network, a deep reinforcement learning approach - Nitin Mukesh)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Deep Q-Network}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl29}
\end{center}

{\tiny (Ref: Deep Learning - MIT 2019)}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{DQN Tricks}

\begin{itemize}
\item Experience Replay: Stores experiences (actions, state transitions, and rewards) and creates mini batches from them for the training process
\item Fixed Target Network: Error calculation includes the target function depends on network parameters and thus changes quickly. Updating it only every 1,000 steps increase stability of training process.
\end{itemize}

Policy Gradient (PG)
\begin{itemize}
\item DQN (off-policy): Approximate Q and infer optimal policy
\item PG (on-policy): Directly optimize policy space
\end{itemize}

\end{frame}


