
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Q-Learning Algorithm}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Goal}

Q-Learning is:

\begin{itemize}
\item Algorithm to solve Optimal Policy in a MDP
\item Learns the optimal Q-value for each State-Action pair
\item Maximizes the expected Value of the total Reward over all steps.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Approaches}
\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{rl27}
\end{center}

{\tiny (Ref: Deep Learning - MIT 2019)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Value Iteration}

\begin{itemize}
\item Bellman Optimality Equation is:

$q_{*}(s,a) = E[R{t+1} + \gamma max_{a'} q_{*}(s',a')]$


\item The Q-Learning algorithm iteratively updates the Q-values for each State-Action pair using the Bellman Equation until the Q-Function coverages to the optimal Q-function $q_*$
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Foraging Agent}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{rl3}
\end{center}

\begin{itemize}
\item Fox wants to get meat but avoid hunters.
\item The whole grid is an Environment, boxes are States and the fox is the agent. Reward is meat/hunter. Actions are up-down-right-left movements.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Rewards}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl3}
\end{center}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here

\begin{itemize}
\item Empty : $-1$ as movement requires some effort
\item One meat : $+1$
\item Two meat : $+2$
\item Three meat : $+3$
\item Hunter : $-5$ game over
\item Maximum reward: game over (ie $+6$)
\end{itemize}

\end{column}
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Q-Value Table}

\begin{columns}
\begin{column}{0.5\textwidth}
Initialized to 0s at the start. Empty boxes are numbered from top and then to right and so on.

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl4}
\end{center}
\end{column}
\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Epsilon Greedy Strategy}


\begin{itemize}
\item Exploration rate $\epsilon$, 1 to start with. Meaning the Agent with Explore (random walk) 100\% of time.
\item At the start of each new episode $\epsilon$ will decay-reduce, then the Agent will become GREEDY and start Exploiting.
\item In the algorithm, generate random number $r$ between 0 and 1.
\item if $r > \epsilon$ the Agent will Exploit (meaning, use past experience based formulation to get the next Action), else it will Explore (meaning, the next Action proposed would be randomly selected from the given set of Actions)
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Move}

\begin{itemize}
\item Fox moves up (say, via random). So, reward is $-1$. 
\item To update the Q-value for the Action of Moving Up taken from the previous State, using Bellman Equation and applying Learning rate $\alpha$ (to control the speed, just like in Gradient Descent).
\item At start, initial q value was 0 and with $\alpha = 0.7, \gamma = 0.99$

\item 

$q^{new}(s,a) = (1 - \alpha)\underbrace{q(s,a)}^\text{old value} + \alpha \overbrace{(R_{t+1} + \gamma max_{a'} q(s',a'))}^\text{learning rate} = (1-0.7)(0) + 0.7(-1 + 0.99(max_{a'} q(s',a')))$ 
\item where, $max_{a'} q(s',a') = max(q(empty8,left), q(empty8, up), q(empty8,down)) = max(-1,1,-1) = 1$ where 'empty8' is the moved position. $+1$ because there is 1 meat there if moves further up.
\item $= (1-0.7)(0) + 0.7(-1, 0.99(1)) = 0 + 0.7(-0.01) = -0.007$
\item Update the Q table and repeat till converges to Optimal Policy (how?)
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Problems}
\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{rl28}
\end{center}

{\tiny (Ref: Deep Learning - MIT 2019)}

Solution for this very High Dimensionality problem is Neural Networks. They approximate the Q-function.

\end{frame}

