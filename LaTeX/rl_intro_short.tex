%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why 'Go' after Reinforcement Learning?}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{rl12}

\includegraphics[width=0.5\linewidth,keepaspectratio]{rl13}

Alpha Go (4) vs Lee Sedol (1)
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Brief History}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl17}
\end{center}

{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is Reinforcement Learning?}
Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{rl9}
\end{center}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Comparison in General}

\begin{itemize}
\item Rule-based System: input is given, you write rules/logic to generate expected output
\item Machine Learning: Input and output, both are given, logic/model is fitted to the data
	\begin{itemize}
	\item Tabular: all rows are independent, can be processed in any order, output is at the end
	\item Sequential: input is in form of sequence and output is at the end, e.g., Sentiment analysis
	\end{itemize}
\item Reinforcement Learning: 
	\begin{itemize}
	\item Sequential decision making 
	\item Problem setup as agent in unknown environment. 
	\item Goal: select action to maximize a future cumulative reward
	\item Machine Learning learns the data, whereas RL learns the Goal
	\end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Comparison with ML}
\begin{center}
\includegraphics[width=0.9\linewidth,keepaspectratio]{rl25}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Comparison wrt Data}

\begin{itemize}
\item If you have labeled data; go for Supervised ML || In $f(X) = y$; given $X$ and $y$,  find $f$.
\item If you have no labels but data; go for Unsupervised ML || Given $X$ and no $y$,  find groups within $X$
\item If you have no data; go for Reinforcement Learning || Given no ready pairs of $X$ and $y$ but a notion of reward $z$, find $f$; $X$ is states, $y$ is action, in RL we are finding policy ie $f(X)$ using a helper quantity $z$.
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Comparison with Humans}

Humans appear to learn to walk through ``very few examples'' of trial and error. How is an open question…Possible answers?:
\begin{itemize}
\item Hardware: 230 million years of bipedal movement data.
\item Imitation Learning: Observation of other humans walking.
\item Algorithms: Better than back-propagation and stochastic gradient descent
\end{itemize}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Adv DisAdv}

Advantages:
\begin{itemize}
\item Less labeled data, almost like bootstrapping
\item Structurally suitable for reward/penalty-based problems
\item As less data, less bias, ethically batter
\end{itemize}

Disadvantages:
\begin{itemize}
\item Take very long to converge, huge compute requirements
\item Does not generalize well. Small change in environment fails it
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RL: High Level Ida}

\begin{itemize}
\item   Goal: Select actions to maximize expected cumulative future reward. (The Returns are $Expected$ as many a times they can be stochastic and not deterministic)
\item   Requires: Balancing short term and long term rewards, Strategy to maximize rewards
\item   Example: Web Advertising
\end{itemize}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RL Definitions}
\begin{itemize}
\item Reinforcement learning, a type of machine learning, in which agents take actions in an environment aimed at maximizing their cumulative rewards – NVIDIA

\item Reinforcement learning (RL) is based on rewarding desired behaviors or punishing undesired ones. Instead of one input producing one output, the algorithm produces a variety of outputs and is trained to select the right one based on certain variables – Gartner

\item It is a type of machine learning technique where a computer agent learns to perform a task through repeated trial and error interactions with a dynamic environment. This learning approach enables the agent to make a series of decisions that maximize a reward metric for the task without human intervention and without being explicitly programmed to achieve the task – Mathworks
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Essentially}
\begin{itemize}
\item Interactive Learning within an Environment, like a student and a teacher
\item Learning by trial and error, with adaptive control
\item Goal is to build a policy-recipe for maximizing reward.
\item Agent interacts with an Environment, with a feedback loop
\item Agent finds optimum policy-strategy-recipe for making decision for better long term rewards.
\end{itemize}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{ML vs RL}

% \begin{itemize}
% \item RL is part of ML
% \item Supervised ML learns the training data, tries to generalize it. But needs large amounts of labeled data.
% \item Unsupervised ML also needs data, but unlabeled. Tries to find groups based on similarity.
% \item RL does not need labeled data. It needs formulation for rewards and an Environment that can be poked. Maximizing Reward is the goal. Does not try to find pattern in the data (well, data is not there anyway).
% \end{itemize}

% \begin{center}
% \includegraphics[width=0.5\linewidth,keepaspectratio]{rl7}
% \end{center}

% {\tiny (Ref: Introduction to Reinforcement Learning for Beginners - Analytics Vidhya)} 

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Characteristics}
\begin{itemize}
\item No supervision, only a real value or reward signal
\item Decision making is sequential
\item Time plays a major role in reinforcement problems
\item Feedback isn’t prompt but delayed
\item The following data it receives is determined by the agent’s actions
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Approaches}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{rl8}

\includegraphics[width=0.4\linewidth,keepaspectratio]{rl15}

\end{center}

{\tiny (Ref: Reinforcement Learning Algorithms – AISummer)} 


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Approaches}

\begin{itemize}
\item Value-Based – The main goal of this method is to maximize a value function. Here, an agent through a policy expects a long-term return of the current states.

\item Policy-Based – In policy-based, you enable to come up with a strategy that helps to gain maximum rewards in the future through possible actions performed in each state. Two types of policy-based methods are deterministic and stochastic.

\item Model-Based – In this method, we need to create a virtual model for the agent to help in learning to perform in each specific environment
\end{itemize}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{rl26}
\end{center}

{\tiny (Ref: Deep Learning - MIT 2019)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Algorithms}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl16}
\end{center}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Algorithms}
% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{rl30}
% \end{center}

% {\tiny (Ref: Deep Learning - MIT 2019)}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Algorithms}

\begin{itemize}
\item Q-learning and SARSA (State-Action-Reward-State-Action): commonly used model-free RL algorithms. 
\item Differ in terms of their exploration strategies while their exploitation strategies are similar. 
\item While Q-learning is an off-policy method in which the agent learns the value based on action a* derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action a derived from its current policy.
\item Both lack generality as they do not have the ability to estimates values for unseen states.
\item Solution:  Deep Q-Networks(DQNs) which use Neural Networks to estimate Q-values. But DQNs can only handle discrete, low-dimensional action spaces.
\item Deep Deterministic Policy Gradient(DDPG) is a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces. The figure below is a representation of actor-critic architecture.
\end{itemize}




\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Types of sequential decision making}

\begin{columns}
\begin{column}{0.5\textwidth}

\begin{itemize}
\item Markov Decision Processes (MDPs and POMDPs)
	\begin{itemize}
	\item Agent’s state is dynamic
	\item Actions influence future observations
	\end{itemize}
\item Bandits
	\begin{itemize}
	\item Agent’s state is fixed
	\item Actions have no influence on next observations
	\end{itemize}
\end{itemize}

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here

\begin{itemize}
\item Multi-arm bandits	
	\begin{itemize}
	\item Task: Choose repeatedly from one of n actions (play)
	\item Objective: optimize long term cumulative reward
	\end{itemize}
\item Contextual bandits	
	\begin{itemize}
	\item Context: extra information that can be used for making better decision when choosing amongst all actions
	\item Example, user history, preferences, etc
	\end{itemize}	
\end{itemize}

\end{column}
\end{columns}




\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Observability}
When Observations are same as States (meaning, everything is fully visible, like in board games) the Environment is called as Fully Observable. If not, meaning, Observations occlude underlying real State, then truth is not fully known, thats Partially Observation Environment

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{rl22}
\end{center}

{\tiny (Ref: But what is Reinforcement Learning? | Reinforcement Learning Part-1 - Rajtilak Pal (M. Tech in AI, IIT Ropar))}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Episodic vs Continuous}

Episodic:
\begin{itemize}
\item Interaction is naturally split into sequence, ie discrete events, eg. Table Tennis
\item Final steps is when point is scored (time of the reward)
\item Next episode begins Independently of how the previous was ended.
\end{itemize}

Continuous:
\begin{itemize}
\item Interaction is naturally smooth ie continuous flow, so no time steps as such.
\item Thus, Reward has to be function and at times even States have to be so.
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exploration vs Exploitation}

Challenges

\begin{itemize}
\item Deterministic/greedy policy won’t explore all actions
	\begin{itemize}
	\item Don’t know anything about the environment at the beginning
	\item Need to try all actions to find the optimal one
	\end{itemize}

\item $\epsilon$-greedy policy
	\begin{itemize}
	\item	With probability $1-\epsilon$ perform the optimal/greedy action, otherwise random action
	\item	Slowly move it towards greedy policy: $\epsilon \rightarrow 0$
	\end{itemize}
\item Agent can decide next Action based on, either
	\begin{itemize}
	\item Experiences so far (called Exploitation), or
	\item just random (called Exploration)
	\end{itemize}
\item Till enough experiences are built, Exploration is used. Also, it can give a random twist, which brings generation, and unexplored paths.
\item Full Exploitation: Greedy Agent
\item Full Exploration: Random
\item Need to find a balance and also when to use what.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Implementation platforms}

\begin{itemize}
\item RLib - Ray Berkley
\item Tf Agents - Google
\item Open AI Gym
\item Project Malmo - Microsoft
\item DeepMind Lab
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Applications}

\begin{itemize}
\item Robotics for Industrial Automation
\item Text summarization engines, dialog agents (text, speech), game-plays
\item Autonomous Self Driving Cars
\item Machine Learning and Data Processing
\item Training system which would issue custom instructions and materials with respect to the requirements of students
\item AI Toolkits, Manufacturing, Automotive, Health-care, and Bots
\item Aircraft Control and Robot Motion Control
\item Building artificial intelligence for computer games
\end{itemize}

\end{frame}

